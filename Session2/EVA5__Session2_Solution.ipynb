{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA5 _Session2_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73KcagIam2bk",
        "colab_type": "text"
      },
      "source": [
        "EVA5 Assignmnet 2\n",
        "\n",
        "Team Members:\n",
        "\n",
        "Smita Sassindran\n",
        "\n",
        "Vishesh Sethi\n",
        "\n",
        "Avneesh Nolkha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztvaply-I8HN",
        "colab_type": "text"
      },
      "source": [
        "Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn                         #Torch NeuralNets\n",
        "import torch.nn.functional as F               # Torch Functions\n",
        "import torch.optim as optim                   # Torch Optimizer\n",
        "from torchvision import datasets, transforms  # Dataset and Transdorms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxYMl6XWI_QE",
        "colab_type": "text"
      },
      "source": [
        "nn.Module: Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\n",
        "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):                                            #Input- rows x columns x channels|| Output- rows x columns x channels || Receptive Field\n",
        "    def __init__(self):                                          #________________________________ \n",
        "        super(Net, self).__init__()                              #|input    || Output   || RF    |\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)              #|28x28x1  || 28x28x32 || 3x3   |                     \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)             #|28x28x32 || 28x28x64 || 5x5   |\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)                          #|28x28x64 || 14x14x64 || 10x10 | \n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)            #|14x14x64 || 14x14x128|| 12x12 |\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)           #|14x14x128|| 14x14x256|| 14x14 |\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)                          #|14x14x256|| 7x7x256  || 28x28 |\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)                      #|7x7x256  || 7x7x512  || 30x30 |\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)                     #|7x7x1024 || 7x7x1024 || 32x32 |\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)                      #|7x7x1024 || 7x7x10   || 34x32 |\n",
        "                                                                 #|______________________________|\n",
        "    \n",
        "    \"Defines computations to be performed everytime the function is called\"\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))        # ReLU(Conv1) -> ReLU(Conv2) -> MaxPool1\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))        # ReLU(Conv3) -> ReLU(Conv4) -> MaxPool2\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))                    # ReLU(Conv5) -> ReLU(Conv6)\n",
        "        x = F.relu(self.conv7(x))                                        # ReLU(Conv7) \n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)                                          #Returns softmax of the final output\n",
        "\n",
        "#Submodules assigned in this way will be registered, and will have their parameters converted too when you call to(), etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfzeTJGLbZsb",
        "colab_type": "text"
      },
      "source": [
        "Torchsummary is used to plot the “forward()” structure in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "0c9d4d7c-b73d-45ac-c369-5416862f4bb9"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "use_cuda = torch.cuda.is_available() # check if nvidia cuda gpu is available\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\") # Switch to CPU if GPU is not availiable\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28)) #Plots model summary or the forward function."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6t6gYHpdfFX",
        "colab_type": "text"
      },
      "source": [
        "# Seeding & Reproduciblity\n",
        "\n",
        "Completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms. Furthermore, results need not be reproducible between CPU and GPU executions, even when using identical seeds.\n",
        "\n",
        "However, in order to make computations deterministic on your specific problem on one specific platform and PyTorch release, there are a couple of steps to take.\n",
        "\n",
        "There are two pseudorandom number generators involved in PyTorch, which you will need to seed manually to make runs reproducible. Furthermore, you should ensure that all other libraries your code relies on and which use random numbers also use a fixed seed.\n",
        "\n",
        "The seed method is used to initialize the pseudorandom number generator in Pytorch.If seed value is not present it takes system current time. if you provide same seed value before generating random data it will produce the same data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1) #Model seeding to obtain consistent result\n",
        "batch_size = 128     #Batch size defines number of samples that going to be propagated through the network in a single go. \n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {} # this makes sure that the data stays in the memory\n",
        "\n",
        "# Load the MNIST dataset and performs Normalization on the each channel of the image data.\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "#Load the test dataset and perform normalization\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm # tqdm Instantly make your loops show a smart progress meter \n",
        "\n",
        "\"\"\"This function trains the model\"\"\"\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)   # move the data to the device\n",
        "        optimizer.zero_grad()                               # Make gradient zero for optimizer\n",
        "        output = model(data)                                # Model output\n",
        "        loss = F.nll_loss(output, target)                   # Loss is Negative Log Likelihood\n",
        "        loss.backward()                                     # Propogate the gradient backward\n",
        " # optimizer.step is performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule    \n",
        "        optimizer.step() \n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')  #make your loops show a smart progress meter\n",
        "\n",
        "\n",
        "\"\"\"This function is for testing the model\"\"\"\n",
        "def test(model, device, test_loader):\n",
        "\n",
        "    # set the model on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # set the test loss to zero\n",
        "    test_loss = 0\n",
        "\n",
        "    # number of correct classifications\n",
        "    correct = 0\n",
        "\n",
        "    # turn off gradients, since we are in test mode\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            # move the data to device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # get the model output\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "f49c0715-fb3a-4356-f0e9-378f2bd1f1fb"
      },
      "source": [
        "# move the model to device\n",
        "model = Net().to(device)\n",
        "# stochastic gradient descent with model parameters, learning rate and momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "# run the model for range number of times\n",
        "for epoch in range(1, 2):\n",
        "    # train the model\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    # test the model\n",
        "    test(model, device, test_loader)\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=2.3025858402252197 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.31it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 1028/10000 (10%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.19270245730876923 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0640, Accuracy: 9787/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}