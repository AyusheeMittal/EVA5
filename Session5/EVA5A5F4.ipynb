{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA5A5F4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZaxUFh-upV8",
        "colab_type": "text"
      },
      "source": [
        "**Team Members**\n",
        "\n",
        "Aayushe Mittal\n",
        "\n",
        "Smita Sassindran\n",
        "\n",
        "Vishesh Sethi\n",
        "\n",
        "Avneesh Nolkha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuUBzX_zurwz",
        "colab_type": "text"
      },
      "source": [
        "**Analysis Block**\n",
        "\n",
        "In this file we need to tweak the LR rate and momentum. \n",
        "\n",
        "`Learning rate`: The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
        "\n",
        "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.\n",
        "\n",
        "`Momentum`: Momentum pushes your output towards global optimum. Too big or too small will ruin everything. In other words, momentum changes the path you take to the optimum. It helps overcome local optimum (If you get stuck).\n",
        "\n",
        "For example: If you have an objective function, you have to decide how to move around on it. Steepest descent on the gradient is the simplest approach, but fluctuations could be a big problem. Adding momentum helps solve that problem.\n",
        "\n",
        "NOTE: High momentum should always be accompanied by low learning rate, else you will overshoot the global optimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pv-t5JpdDS0",
        "colab_type": "text"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGRJvQsUc3DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GI9sAFdIQJ",
        "colab_type": "text"
      },
      "source": [
        "Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX9nSD-C_e2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                      transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz8iGrH0dUxY",
        "colab_type": "text"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9UB_t0s_pKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bblr6MqYdXm_",
        "colab_type": "text"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja6Yrcq9_w22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b37437c4-e92b-43ee-8271-27677d6d51c9"
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io7IDVsvdhVB",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5BNiZvn_4Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Block 1\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10)\n",
        "            \n",
        "        ) # output_size = 26\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10)\n",
        "        ) # output_size = 24\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16)\n",
        "        ) # output_size = 22\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=12, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12)\n",
        "        ) # output_size = 11\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=12, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16)\n",
        "        ) # output_size = 9\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(20)\n",
        "        ) # output_size = 7\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=7)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=20, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16)\n",
        "        ) # output_size = 7\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 7\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.convblock8(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlckNRvDdoD5",
        "colab_type": "text"
      },
      "source": [
        "Model Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVyKicBtdrEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "dbc95dab-8df7-43ed-9cca-0941ac770972"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]              90\n",
            "              ReLU-2           [-1, 10, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 10, 26, 26]              20\n",
            "            Conv2d-4           [-1, 10, 24, 24]             900\n",
            "              ReLU-5           [-1, 10, 24, 24]               0\n",
            "       BatchNorm2d-6           [-1, 10, 24, 24]              20\n",
            "            Conv2d-7           [-1, 16, 22, 22]           1,440\n",
            "              ReLU-8           [-1, 16, 22, 22]               0\n",
            "       BatchNorm2d-9           [-1, 16, 22, 22]              32\n",
            "        MaxPool2d-10           [-1, 16, 11, 11]               0\n",
            "           Conv2d-11           [-1, 12, 11, 11]             192\n",
            "             ReLU-12           [-1, 12, 11, 11]               0\n",
            "      BatchNorm2d-13           [-1, 12, 11, 11]              24\n",
            "           Conv2d-14             [-1, 16, 9, 9]           1,728\n",
            "             ReLU-15             [-1, 16, 9, 9]               0\n",
            "      BatchNorm2d-16             [-1, 16, 9, 9]              32\n",
            "           Conv2d-17             [-1, 20, 7, 7]           2,880\n",
            "             ReLU-18             [-1, 20, 7, 7]               0\n",
            "      BatchNorm2d-19             [-1, 20, 7, 7]              40\n",
            "        AvgPool2d-20             [-1, 20, 1, 1]               0\n",
            "           Conv2d-21             [-1, 16, 1, 1]             320\n",
            "             ReLU-22             [-1, 16, 1, 1]               0\n",
            "      BatchNorm2d-23             [-1, 16, 1, 1]              32\n",
            "           Conv2d-24             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 7,910\n",
            "Trainable params: 7,910\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.56\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.60\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWo5Ny6aduIC",
        "colab_type": "text"
      },
      "source": [
        "Training and Testing Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57yWDgg6BBHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader, position=0)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkANIufeBDAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d8bac16-542e-4ae0-87ed-e6f632207a03"
      },
      "source": [
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0180, momentum=0.92)\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch+1)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.03220159932971001 Batch_id=468 Accuracy=92.49: 100%|██████████| 469/469 [00:14<00:00, 32.86it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0707, Accuracy: 9776/10000 (97.76%)\n",
            "\n",
            "EPOCH: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.21457858383655548 Batch_id=468 Accuracy=97.99: 100%|██████████| 469/469 [00:13<00:00, 33.69it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0432, Accuracy: 9863/10000 (98.63%)\n",
            "\n",
            "EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.03968654200434685 Batch_id=468 Accuracy=98.40: 100%|██████████| 469/469 [00:13<00:00, 33.93it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0306, Accuracy: 9906/10000 (99.06%)\n",
            "\n",
            "EPOCH: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.008164864964783192 Batch_id=468 Accuracy=98.70: 100%|██████████| 469/469 [00:13<00:00, 34.22it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "EPOCH: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04164893925189972 Batch_id=468 Accuracy=98.84: 100%|██████████| 469/469 [00:13<00:00, 34.38it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0322, Accuracy: 9893/10000 (98.93%)\n",
            "\n",
            "EPOCH: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.023164262995123863 Batch_id=468 Accuracy=98.90: 100%|██████████| 469/469 [00:13<00:00, 34.19it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0269, Accuracy: 9910/10000 (99.10%)\n",
            "\n",
            "EPOCH: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.00391644099727273 Batch_id=468 Accuracy=99.20: 100%|██████████| 469/469 [00:13<00:00, 34.76it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0201, Accuracy: 9933/10000 (99.33%)\n",
            "\n",
            "EPOCH: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.008501308038830757 Batch_id=468 Accuracy=99.33: 100%|██████████| 469/469 [00:13<00:00, 34.91it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 9933/10000 (99.33%)\n",
            "\n",
            "EPOCH: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.00463935686275363 Batch_id=468 Accuracy=99.32: 100%|██████████| 469/469 [00:13<00:00, 34.69it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "EPOCH: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.01934104971587658 Batch_id=468 Accuracy=99.36: 100%|██████████| 469/469 [00:13<00:00, 35.15it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0191, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "EPOCH: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.039159465581178665 Batch_id=468 Accuracy=99.37: 100%|██████████| 469/469 [00:13<00:00, 35.13it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04456345736980438 Batch_id=468 Accuracy=99.40: 100%|██████████| 469/469 [00:13<00:00, 35.15it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "EPOCH: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.031308505684137344 Batch_id=468 Accuracy=99.43: 100%|██████████| 469/469 [00:13<00:00, 35.68it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 9942/10000 (99.42%)\n",
            "\n",
            "EPOCH: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.008282930590212345 Batch_id=468 Accuracy=99.45: 100%|██████████| 469/469 [00:13<00:00, 35.87it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0193, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "EPOCH: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.011494937352836132 Batch_id=468 Accuracy=99.49: 100%|██████████| 469/469 [00:13<00:00, 35.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0188, Accuracy: 9943/10000 (99.43%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Do2NgYvjDo",
        "colab_type": "text"
      },
      "source": [
        "**Analysis Block**\n",
        "\n",
        "Target Accuracy: 99.4\n",
        "\n",
        "Target Params: < 8k\n",
        "\n",
        "Current Accuracy: 99.4\n",
        "\n",
        "Current Params: 7910\n"
      ]
    }
  ]
}