{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA5A5F4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZaxUFh-upV8",
        "colab_type": "text"
      },
      "source": [
        "**Team Members**\n",
        "\n",
        "Ayushee Mittal\n",
        "\n",
        "Smita Sasindran\n",
        "\n",
        "Vishesh Sethi\n",
        "\n",
        "Avneesh Nolkha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuUBzX_zurwz",
        "colab_type": "text"
      },
      "source": [
        "**Analysis Block**\n",
        "\n",
        "In this file we need to tweak the LR rate and momentum. \n",
        "\n",
        "`Learning rate`: The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs.\n",
        "\n",
        "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.\n",
        "\n",
        "`Momentum`: Momentum pushes your output towards global optimum. Too big or too small will ruin everything. In other words, momentum changes the path you take to the optimum. It helps overcome local optimum (If you get stuck).\n",
        "\n",
        "For example: If you have an objective function, you have to decide how to move around on it. Steepest descent on the gradient is the simplest approach, but fluctuations could be a big problem. Adding momentum helps solve that problem.\n",
        "\n",
        "NOTE: High momentum should always be accompanied by low learning rate, else you will overshoot the global optimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pv-t5JpdDS0",
        "colab_type": "text"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGRJvQsUc3DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2GI9sAFdIQJ",
        "colab_type": "text"
      },
      "source": [
        "Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX9nSD-C_e2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                      transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz8iGrH0dUxY",
        "colab_type": "text"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9UB_t0s_pKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bblr6MqYdXm_",
        "colab_type": "text"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja6Yrcq9_w22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b37437c4-e92b-43ee-8271-27677d6d51c9"
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io7IDVsvdhVB",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5BNiZvn_4Bn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Block 1\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10)\n",
        "            \n",
        "        ) # output_size = 26\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10)\n",
        "        ) # output_size = 24\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16)\n",
        "        ) # output_size = 22\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 11\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=12, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(12)\n",
        "        ) # output_size = 11\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=12, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16)\n",
        "        ) # output_size = 9\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=20, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(20)\n",
        "        ) # output_size = 7\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=7)\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=20, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16)\n",
        "        ) # output_size = 7\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # output_size = 7\n",
        "\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.convblock8(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlckNRvDdoD5",
        "colab_type": "text"
      },
      "source": [
        "Model Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVyKicBtdrEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "78f67b6b-92e0-4349-8762-1a983cbc0005"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]              90\n",
            "              ReLU-2           [-1, 10, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 10, 26, 26]              20\n",
            "            Conv2d-4           [-1, 10, 24, 24]             900\n",
            "              ReLU-5           [-1, 10, 24, 24]               0\n",
            "       BatchNorm2d-6           [-1, 10, 24, 24]              20\n",
            "            Conv2d-7           [-1, 16, 22, 22]           1,440\n",
            "              ReLU-8           [-1, 16, 22, 22]               0\n",
            "       BatchNorm2d-9           [-1, 16, 22, 22]              32\n",
            "        MaxPool2d-10           [-1, 16, 11, 11]               0\n",
            "           Conv2d-11           [-1, 12, 11, 11]             192\n",
            "             ReLU-12           [-1, 12, 11, 11]               0\n",
            "      BatchNorm2d-13           [-1, 12, 11, 11]              24\n",
            "           Conv2d-14             [-1, 16, 9, 9]           1,728\n",
            "             ReLU-15             [-1, 16, 9, 9]               0\n",
            "      BatchNorm2d-16             [-1, 16, 9, 9]              32\n",
            "           Conv2d-17             [-1, 20, 7, 7]           2,880\n",
            "             ReLU-18             [-1, 20, 7, 7]               0\n",
            "      BatchNorm2d-19             [-1, 20, 7, 7]              40\n",
            "        AvgPool2d-20             [-1, 20, 1, 1]               0\n",
            "           Conv2d-21             [-1, 16, 1, 1]             320\n",
            "             ReLU-22             [-1, 16, 1, 1]               0\n",
            "      BatchNorm2d-23             [-1, 16, 1, 1]              32\n",
            "           Conv2d-24             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 7,910\n",
            "Trainable params: 7,910\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.56\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.60\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWo5Ny6aduIC",
        "colab_type": "text"
      },
      "source": [
        "Training and Testing Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57yWDgg6BBHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader, position=0)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkANIufeBDAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c99f734c-9da7-4ced-b9ae-1a5c3f921696"
      },
      "source": [
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.025, momentum=0.95)\n",
        "scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch+1)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "    \"\"\"After successful run we realised, that as lr keeps increasing, aftyer few epochs it starts causing overfitting. \n",
        "    So we are putting a condition on LR that it won't increase if overfitting occurs.\n",
        "    \"\"\"\n",
        "    if train_acc[epoch] < test_acc[epoch] :\n",
        "      scheduler.step()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.059719111770391464 Batch_id=468 Accuracy=93.78: 100%|██████████| 469/469 [00:14<00:00, 33.33it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0558, Accuracy: 9813/10000 (98.13%)\n",
            "\n",
            "EPOCH: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04900708422064781 Batch_id=468 Accuracy=97.99: 100%|██████████| 469/469 [00:13<00:00, 33.94it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 9860/10000 (98.60%)\n",
            "\n",
            "EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.05546626076102257 Batch_id=468 Accuracy=98.45: 100%|██████████| 469/469 [00:13<00:00, 33.70it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9890/10000 (98.90%)\n",
            "\n",
            "EPOCH: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04864649847149849 Batch_id=468 Accuracy=98.59: 100%|██████████| 469/469 [00:13<00:00, 34.31it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "EPOCH: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.006305094808340073 Batch_id=468 Accuracy=98.72: 100%|██████████| 469/469 [00:13<00:00, 34.48it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "EPOCH: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.028734110295772552 Batch_id=468 Accuracy=98.90: 100%|██████████| 469/469 [00:13<00:00, 34.69it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 9896/10000 (98.96%)\n",
            "\n",
            "EPOCH: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.044065069407224655 Batch_id=468 Accuracy=99.21: 100%|██████████| 469/469 [00:13<00:00, 34.48it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.008244984783232212 Batch_id=468 Accuracy=99.28: 100%|██████████| 469/469 [00:13<00:00, 34.36it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0192, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "EPOCH: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.016735153272747993 Batch_id=468 Accuracy=99.33: 100%|██████████| 469/469 [00:13<00:00, 34.24it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0182, Accuracy: 9944/10000 (99.44%)\n",
            "\n",
            "EPOCH: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.021265000104904175 Batch_id=468 Accuracy=99.34: 100%|██████████| 469/469 [00:13<00:00, 33.67it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "EPOCH: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.018352627754211426 Batch_id=468 Accuracy=99.36: 100%|██████████| 469/469 [00:13<00:00, 34.03it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "EPOCH: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.11063512414693832 Batch_id=468 Accuracy=99.37: 100%|██████████| 469/469 [00:13<00:00, 34.29it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "EPOCH: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.007702527567744255 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:13<00:00, 34.11it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0178, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "EPOCH: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.00826883502304554 Batch_id=468 Accuracy=99.41: 100%|██████████| 469/469 [00:13<00:00, 34.08it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0180, Accuracy: 9942/10000 (99.42%)\n",
            "\n",
            "EPOCH: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.023891210556030273 Batch_id=468 Accuracy=99.42: 100%|██████████| 469/469 [00:13<00:00, 34.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0175, Accuracy: 9942/10000 (99.42%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Do2NgYvjDo",
        "colab_type": "text"
      },
      "source": [
        "**Analysis Block**\n",
        "\n",
        "Target Accuracy: 99.4\n",
        "\n",
        "Target Params: < 8k\n",
        "\n",
        "Current Accuracy: 99.4\n",
        "\n",
        "Current Params: 7910\n"
      ]
    }
  ]
}